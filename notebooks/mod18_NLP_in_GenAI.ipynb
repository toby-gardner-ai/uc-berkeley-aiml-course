{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5ab7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [Errno 8] nodename nor servname provided, or not\n",
      "[nltk_data]     known>\n"
     ]
    }
   ],
   "source": [
    "# Data Analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Collection\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#NLP - Modern\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import bm25s\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#NLP - Classic\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "# ML/Vector Search\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Input/Query Validation\n",
    "import base64\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b539b2d",
   "metadata": {},
   "source": [
    "# Building GenAI applications using NLP\n",
    "\n",
    "In this module, I'm going to go through how you can apply multiple NLP techniques as part of building a GenAI application. In this example, I'm going to\n",
    "\n",
    "**Data Source**\n",
    "Description                 |   Objective                           |   NLP\n",
    "Collect webpage data        |   Guardrail: HTTPS, no redirect       |   Regex\n",
    "Analyse Data                |   Understand content                  |   Bag of words, TD-IDF\n",
    "Data Prep (Chunking)        |   Split into sentences                |   \n",
    "Data Prep (Tokenisation)    |   Convert into words                  |\n",
    "Feature Eng'g (Embedding)   |                                       |\n",
    "Feature Eng'g (Indexing)    |   \n",
    "\n",
    "**Keyword v. Semantic Retrieval**\n",
    "Description                 |   Objective                           |   NLP\n",
    "Semantic Retrieval          |   Similar chunks-to-query             |   KNN\n",
    "Keyword Retrieval           |   Keyword/lexical chunks-to-query     |   TF-IDF, BM25\n",
    "\n",
    "**User Query**\n",
    "Description                 |   Objective                           |   NLP\n",
    "User Query                  |   Guardrail: Prompt Attack            |   Regex\n",
    "Query Decomposition         |   Improve search by retrieval method  |   POS/NER\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b07ea",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "In GenAI Applications, data security is arguably the single largest risk to enterprise. Therefore, controlling data in/out of any AI/ML pipeline is critical for security, scalability, and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68329eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_url(url):\n",
    "    pattern = re.compile(\n",
    "        # protocol + domain/subdomain + .com etc. + specific pages/paths\n",
    "    r\"^https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(/[a-zA-Z0-9\\-._~:/?#\\[\\]@!$&()*+,;=%]*)?$\"\n",
    ")\n",
    "    if not pattern.fullmatch(url):\n",
    "        return \"Invalid URL address. Please try again\"\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1acd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approved_url(url):\n",
    "\n",
    "    approved_domains = {\"calbears.com\", \"berkeley.edu\"}\n",
    "    disallowed_extensions = {\".json\", \".xml\", \".csv\", \".zip\", \".exe\", \".tar\", \".gz\"}\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # 1. Enforce HTTPS\n",
    "    if parsed.scheme != \"https\":\n",
    "        return \"Unsecure protocol. HTTPS only\"\n",
    "    \n",
    "    # 2. Check approved domains\n",
    "    hostname = parsed.hostname or \"\"\n",
    "    domain_allowed = False\n",
    "\n",
    "    for domain in approved_domains:\n",
    "        if hostname == domain or hostname.endswith(f\".{domain}\"):\n",
    "            domain_allowed = True\n",
    "            break\n",
    "\n",
    "    if not domain_allowed:\n",
    "        return \"This domain is not approved for data retrieval\"\n",
    "\n",
    "    # 3. Block disallowed file formats\n",
    "    _, ext = os.path.splitext(parsed.path.lower())\n",
    "    if ext in disallowed_extensions:\n",
    "        return \"This data format is not approved for data retrieval\"\n",
    "\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeacb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webpage_to_df(url):\n",
    "    # 1) Validate URL format and policy\n",
    "    check_valid = valid_url(url)\n",
    "    if check_valid is not True:\n",
    "        return check_valid  \n",
    "\n",
    "    check_approvals = approved_url(url)\n",
    "    if check_approvals is not True:\n",
    "        return check_approvals  \n",
    "\n",
    "    # 2) Fetch Data\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Failed to retrieve webpage: {e}\"\n",
    "\n",
    "    # 3) Parse + extract text\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Note: Site Specific (would need to standardise this in future - TBC)\n",
    "    article_body = soup.find('div', class_='article__content')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "    else:\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "    text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "    # 4) Sentence split\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    # 5) DataFrame\n",
    "    df = pd.DataFrame({'id': range(1, len(sentences) + 1),'sentence': sentences})\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8434aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://calbears.com/news/2025/12/4/tosh-lupoi-named-travers-family-head-football-coach.aspx\"\n",
    "\n",
    "df = webpage_to_df(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda10452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tosh Lupoi, a former California defensive line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lupoi comes home to Berkeley to become the 35t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence\n",
       "0   1  Tosh Lupoi, a former California defensive line...\n",
       "1   2  Lupoi comes home to Berkeley to become the 35t..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3113b",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Let’s do a quick, visual sanity-check on our scraped article text by generating two Word Clouds from the same sentence DataFrame:\n",
    "\n",
    "**1) Bag of Words (BoW)**\n",
    "\n",
    "- Treats the text as a simple collection of words (no word order, no context), highlighting the most frequently occurring\n",
    "- BOW Matrix: matrix values indicate how many times each word appears in each document.\n",
    "\n",
    "**2) TF-IDF (Term Frequency–Inverse Document Frequency)**\n",
    "\n",
    "- Like BOW, it counts the words and ignores word order BUT gives more weight to words that appear frequently across sentence/chunk but are less frequent/rare across the whole document/corpus\n",
    "- TFIDF Matrix: matrix values indicate the importance of each word in a given sentence/chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41847f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b36220888545f2b03a10cf3d6446e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04d67e18e4b41e3a3ab4d236fd6529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reconstructing token strings:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains ~ 786 words\n",
      "Corpus contains ~ 400 unique words\n"
     ]
    }
   ],
   "source": [
    "corpus = df[\"sentence\"].astype(str).tolist()\n",
    "word_tokens = bm25s.tokenize(texts=corpus, return_ids=False)\n",
    "\n",
    "total_words = [word for sentence in word_tokens for word in sentence]\n",
    "print(f\"Corpus contains ~ {len(total_words)} words\")\n",
    "unique_words = set(total_words)\n",
    "print(f\"Corpus contains ~ {len(unique_words)} unique words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43b7151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(df, method = \"bow\", content_col = \"sentence\", max_words: int = 20):\n",
    "    \n",
    "    corpus = df[content_col].astype(str).tolist()\n",
    "\n",
    "    # Choose Text Vectorisation Strategy\n",
    "    if method == \"bow\":\n",
    "        # Bag of Words: counts raw word occurrences\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    elif method == \"tfidf\":\n",
    "        # TF-IDF: count and weight words according to their 'importance'\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'bow' or 'tfidf'\")\n",
    "\n",
    "    # Fit and transform corpus into a matrix (no. sentences x ~no. unique words in corpus))\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    print(f'{method} Matrix Shape (no. sentences x no. unique words in corpus): ', matrix.get_shape())\n",
    "    \n",
    "    # Map words to corpus-level importance\n",
    "    # BoW uses raw frequency; TF-IDF downweights common words\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    weights = matrix.sum(axis=0).A1\n",
    "\n",
    "    word_freq = dict(zip(words, weights))\n",
    "    \n",
    "\n",
    "    # 4. Visualise Word Cloud from weighted word frequencies\n",
    "    wc = WordCloud(\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        max_words=max_words,\n",
    "        background_color=\"white\",\n",
    "        collocations=False,\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "\n",
    "    # 5. Render Word Cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Corpus Word Cloud ({method.upper()}, Top {max_words})\", fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d3c0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_cloud(df, method=\"bow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "794b9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_cloud(df, method=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2e51c",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0f1d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def load_embedding_model(model_name):\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    modules = list(model._modules.keys())\n",
    "    for key in modules:\n",
    "        if model._modules[key].__class__.__name__ == \"Normalize\":\n",
    "            del model._modules[key]\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad428aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension size per sentence/chunk (set by model): 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85fbb30540a4e409ed6b66681c2757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape (no. sentences/chunks x model's embedding dimensions): (45, 384)\n",
      "Embedding dimension size per sentence/chunk (set by model): 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64b09a3761f4e798d27c8c09ca34c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape (no. sentences/chunks x model's embedding dimensions): (45, 384)\n"
     ]
    }
   ],
   "source": [
    "def embed_matrix(df, model_name, method = \"cos\", content_col = \"sentence\", batch_size=16):\n",
    "    \n",
    "    df = df.copy()\n",
    "    chunks = df[content_col].astype(str).tolist()\n",
    "\n",
    "    embed_model = load_embedding_model(model_name)\n",
    "    embedding_dim = embed_model.get_sentence_embedding_dimension()\n",
    "    print(\"Embedding dimension size per sentence/chunk (set by model):\", embedding_dim)\n",
    "\n",
    "    embed_matrix = embed_model.encode(\n",
    "        chunks,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=False,\n",
    "        show_progress_bar=True\n",
    "    ).astype(\"float32\")\n",
    "    print(\"Embedding Matrix Shape (no. sentences/chunks x model's embedding dimensions):\", embed_matrix.shape)\n",
    "    \n",
    "    if method == 'cos':\n",
    "        norms = np.linalg.norm(embed_matrix, axis=1, keepdims=True)\n",
    "        embed_matrix = embed_matrix / norms\n",
    "        df[\"embedding_cos\"] = list(embed_matrix)\n",
    "\n",
    "    elif method == 'dot':\n",
    "        df[\"embedding_dot\"] = list(embed_matrix)\n",
    "        \n",
    "\n",
    "    return df, embed_matrix\n",
    "\n",
    "df_cos, cos_matrix = embed_matrix(df, model_name)\n",
    "df_dot, dot_matrix = embed_matrix(df, model_name, method='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "827525a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(user_query, method=\"cos\", knn=5):\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    embed_model = load_embedding_model(model_name)\n",
    "    q_raw = embed_model.encode(\n",
    "        [user_query],\n",
    "        normalize_embeddings=False,\n",
    "        show_progress_bar=False\n",
    "    ).astype(\"float32\")[0]  # (embedding_dim,)\n",
    "\n",
    "    if method == \"cos\":\n",
    "        # Cosine search expects L2-normalized vectors\n",
    "        q_norm = np.linalg.norm(q_raw)\n",
    "        q_emb = q_raw / max(q_norm, 1e-12)\n",
    "\n",
    "        sims_all = (cos_matrix @ q_emb).astype(\"float32\")          \n",
    "        dists_all = (1.0 - sims_all).astype(\"float32\")             \n",
    "        top_idx = np.argsort(-sims_all)[:knn]\n",
    "        top_scores_raw = sims_all[top_idx]\n",
    "\n",
    "    elif method == \"dot\":\n",
    "        sims_all = (dot_matrix @ q_raw).astype(\"float32\")           \n",
    "        top_idx = np.argsort(-sims_all)[:knn]\n",
    "        top_scores_raw = sims_all[top_idx]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'cos' or 'dot'\")\n",
    "\n",
    "\n",
    "    sims_arr = sims_all.reshape(-1, 1)\n",
    "    if float(sims_arr.std()) == 0.0:\n",
    "        sims_scaled_all = np.zeros_like(sims_arr, dtype=\"float32\").ravel()\n",
    "    else:\n",
    "        sims_scaled_all = StandardScaler().fit_transform(sims_arr).astype(\"float32\").ravel()\n",
    "\n",
    "    top_scores_scaled = sims_scaled_all[top_idx]\n",
    "\n",
    "    latency_ms = round((time.perf_counter() - start) * 1000.0, 1)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"sentence_id\": df.iloc[top_idx][\"id\"].values,\n",
    "        \"sentence\": df.iloc[top_idx][\"sentence\"].values,\n",
    "        \"rank\": np.arange(1, len(top_idx) + 1),\n",
    "        \"score_scaled\": top_scores_scaled.astype(\"float32\"),\n",
    "        \"method\": method,\n",
    "        \"latency_ms\": latency_ms,\n",
    "    })\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5d649",
   "metadata": {},
   "source": [
    "## Keyword Search\n",
    "- TF-IDF\n",
    "- BM25: \"Best matching\" i.e. TFIDF but normalises for long sentences/corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c48f9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 4)\n",
      "(45, 351)\n"
     ]
    }
   ],
   "source": [
    "def tf_idf_matrix(df, corpus_col):\n",
    "    print(df.shape)\n",
    "    corpus = df[corpus_col].astype(str).tolist()\n",
    "    vectorizer= TfidfVectorizer(stop_words=\"english\")\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    print(matrix.shape)\n",
    "    return matrix, vectorizer\n",
    "\n",
    "tfidf_matrix, tfidf_vectorizer = tf_idf_matrix(df, \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d05f21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_search(query, df, k: int = 5):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    #Vectorise Query\n",
    "    query_vector = tfidf_vectorizer.transform([str(query)])\n",
    "    \n",
    "    scores = (query_vector @ tfidf_matrix.T).toarray().ravel().astype(\"float32\")\n",
    "\n",
    "    scores_arr = scores.reshape(-1, 1)\n",
    "    if scores_arr.std() == 0:\n",
    "        scores_scaled = np.zeros_like(scores_arr).ravel()\n",
    "    else:\n",
    "        scores_scaled = StandardScaler().fit_transform(scores_arr).ravel()\n",
    "\n",
    "    top_idx = np.argsort(-scores)[:k]\n",
    "    latency_ms = round((time.perf_counter() - start) * 1000, 1)\n",
    "\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(top_idx, start=1):\n",
    "        rows.append({\n",
    "            \"sentence_id\": df.iloc[idx][\"id\"],\n",
    "            \"sentence\": df.iloc[idx][\"sentence\"],\n",
    "            \"rank\": int(rank),\n",
    "            \"score_scaled\": float(scores_scaled[idx]),\n",
    "            \"method\": \"tfidf\",\n",
    "            \"latency_ms\": float(latency_ms),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51a40645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aea0e49f87a4fe28e61e93c234b1434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b74880058245edb8f0ae6337038d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0061fd7550e49bd8e56a1f1814a5ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_bm25(df, corpus_col):\n",
    "    print(df.shape)\n",
    "    corpus = df[corpus_col].astype(str).tolist()\n",
    "    \n",
    "    corpus_tokens = bm25s.tokenize(corpus, stopwords='english')\n",
    "    bm25_algo = bm25s.BM25()\n",
    "    bm25_algo.index(corpus_tokens)\n",
    "    return bm25_algo\n",
    "\n",
    "bm25_matrix = build_bm25(df, \"sentence\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d78e0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query, df, k=5):\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # tokenize query\n",
    "    query_tokens = bm25s.tokenize([query], stopwords='english')\n",
    "\n",
    "    doc_ids, scores = bm25_matrix.retrieve(query_tokens, k=k)\n",
    "\n",
    "    latency_ms = round((time.perf_counter() - start) * 1000, 1)\n",
    "\n",
    "    doc_ids = np.asarray(doc_ids)[0]\n",
    "    scores = np.asarray(scores)[0].astype(\"float32\")\n",
    "\n",
    "    # scale scores\n",
    "    scores_arr = scores.reshape(-1, 1)\n",
    "    if scores_arr.std() == 0:\n",
    "        scores_scaled = np.zeros_like(scores_arr).flatten()\n",
    "    else:\n",
    "        scores_scaled = StandardScaler().fit_transform(scores_arr).flatten()\n",
    "\n",
    "    records = []\n",
    "    for rank, (idx, score_scaled) in enumerate(zip(doc_ids, scores_scaled), start=1):\n",
    "        idx = int(idx)\n",
    "        records.append({\n",
    "            \"sentence_id\": df.iloc[idx][\"id\"],\n",
    "            \"sentence\": df.iloc[idx][\"sentence\"],\n",
    "            \"rank\": int(rank),\n",
    "            \"score_scaled\": float(score_scaled),   \n",
    "            \"method\": \"bm25\",\n",
    "            \"latency_ms\": float(latency_ms),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c13032",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "173ef5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInput(BaseModel):\n",
    "    query: str = Field(min_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e12242c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, can\n",
      "1 validation error for UserInput\n",
      "query\n",
      "  String should have at least 10 characters [type=string_too_short, input_value='hi, can', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_too_short\n"
     ]
    }
   ],
   "source": [
    "query_test = \"hi, can\"\n",
    "\n",
    "print(query_test)\n",
    "try:\n",
    "    UserInput(query=query_test)\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82f61a",
   "metadata": {},
   "source": [
    "query = \"Did Tosh Lupoi ever coach a bird?\"\n",
    "query = \"Did Tosh Lupoi ever coach in Georgia?\"\n",
    "query = \"Has he been to Atlanta?\"\n",
    "query = \"Did Tosh Lupoi ever coach in Atlanta?\"\n",
    "query = \"Has he ever coached in Atlanta?\"\n",
    "query = \"coach of atlanta falcons\"\n",
    "query = \"has he coached with the Atlanta Falcons\"\n",
    "\n",
    "Answer can be found in sentence_id = [3, 26, 27]\n",
    "4 = Lupoi has additionally spent time on the coaching staff at the University of Washington and in the NFL with the Cleveland Browns, **Atlanta Falcons** and Jacksonville Jaguars.\n",
    "27 = Prior to his one-year stint with the Jaguars, Lupoi served as the defensive line coach and run game coordinator in **Atlanta**.\n",
    "28 = During his time with the Falcons, **Atlanta** allowed the sixth-fewest rushing yards in the NFL (1,677) and tied for seventh-fewest first downs on the ground (97)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51515c",
   "metadata": {},
   "source": [
    "### Query Decomposition\n",
    "- Using NER/POS to identify keywords such as nouns to assist keyword searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e077bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(query):\n",
    "\n",
    "    words = word_tokenize(query)\n",
    "\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    pos_prefix = (\"NN\")\n",
    "\n",
    "    filtered_words = [word for word, tag in pos_tags if tag.startswith(pos_prefix)]\n",
    "\n",
    "    pos_query = \" \".join(filtered_words)\n",
    "    \n",
    "    return query, pos_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4069b8",
   "metadata": {},
   "source": [
    "Answer can be found in sentence_id = [4, 27, 28]\n",
    "\n",
    "**Keyword Search Strength**\n",
    "raw_query = \"did Tosh Lupoi coach with the Atlanta Falcons\" --> perfect TFIDF/BM25\n",
    "\n",
    "**Comparable**\n",
    "raw_query = \"Do you know if Lupoi ever been to the great city of Atlanta?\"\n",
    "- Keyword retriever's outperform when pos_query turned on\n",
    "\n",
    "**Semantic Search Strength 2/3 Cosine, 0 from Keyword retrievers**\n",
    "raw_query = \"Do you know if Lupoi ever coached for a team named after a bird!\"\n",
    "raw_query = \"Do you know if Lupoi ever been to the great city of Atalanta?\"\n",
    "raw_query = \"Do you know if Lupoi ever coached in a state near Florida?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57178c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a7768efc494429b4a24830ce30bdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e6e766128a48e284a42e24e3827c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_query = \"did Tosh Lupoi coach with the Atlanta Falcons\"\n",
    "\n",
    "try:\n",
    "    UserInput(query=raw_query)\n",
    "except ValidationError as e:\n",
    "    print(e)\n",
    "\n",
    "query, pos_query = decompose_query(raw_query)\n",
    "\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos = semantic_search(query, method=\"cos\", knn=3)\n",
    "test_dot = semantic_search(query, method=\"dot\", knn=3)\n",
    "test_bm25 = bm25_search(query, df, k=3)\n",
    "test_tfidf = tfidf_search(query, df, k=3)\n",
    "test_results = pd.concat([test_cos, test_dot,test_bm25, test_tfidf], ignore_index=True)\n",
    "test_results[\"correct\"] = (test_results[\"sentence_id\"].isin([4, 27, 28]).map({True: \"Correct\", False: \"-\"}))\n",
    "\n",
    "test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
