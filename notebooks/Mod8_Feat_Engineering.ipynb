{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df1668",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In our last session, we expanded on the curriculum by looking deeper on how to apply unsupervised learning techniques such as DBScan Clustering and PCA not only for EDA and dimensionality reduction but also on how we can create new features aka feature engineering. \n",
    "\n",
    "In this session, we're going to explore Feature Engineering for categorical features and text-based data: preparing you not only in traditional ML but also for Generative AI such as LLMs.\n",
    "\n",
    "Before we dive into tokenization for LLMs, letâ€™s revisit how we handle feature encoding for categorical data â€” a crucial step that parallels what LLMs later do implicitly with text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6dc0d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love everything involving machine learning and the University of California, Berkeley!\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "# dict.fromkeys() --> keeps order and the first time each word appears\n",
    "unique_words = list(dict.fromkeys(words))\n",
    "\n",
    "df = pd.DataFrame(unique_words, columns=['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723dbb61",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "IF you have categorical data with NO inherent order (e.g., colors, countries) â†’ Use one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4e566be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>is_i</th>\n",
       "      <th>is_love</th>\n",
       "      <th>is_everything</th>\n",
       "      <th>is_involving</th>\n",
       "      <th>is_machine</th>\n",
       "      <th>is_learning</th>\n",
       "      <th>is_and</th>\n",
       "      <th>is_the</th>\n",
       "      <th>is_university</th>\n",
       "      <th>is_of</th>\n",
       "      <th>is_california,</th>\n",
       "      <th>is_berkeley!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>learning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California,</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berkeley!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  is_i  is_love  is_everything  is_involving  is_machine  \\\n",
       "0             I     1        0              0             0           0   \n",
       "1          love     0        1              0             0           0   \n",
       "2    everything     0        0              1             0           0   \n",
       "3     involving     0        0              0             1           0   \n",
       "4       machine     0        0              0             0           1   \n",
       "5      learning     0        0              0             0           0   \n",
       "6           and     0        0              0             0           0   \n",
       "7           the     0        0              0             0           0   \n",
       "8    University     0        0              0             0           0   \n",
       "9            of     0        0              0             0           0   \n",
       "10  California,     0        0              0             0           0   \n",
       "11    Berkeley!     0        0              0             0           0   \n",
       "\n",
       "    is_learning  is_and  is_the  is_university  is_of  is_california,  \\\n",
       "0             0       0       0              0      0               0   \n",
       "1             0       0       0              0      0               0   \n",
       "2             0       0       0              0      0               0   \n",
       "3             0       0       0              0      0               0   \n",
       "4             0       0       0              0      0               0   \n",
       "5             1       0       0              0      0               0   \n",
       "6             0       1       0              0      0               0   \n",
       "7             0       0       1              0      0               0   \n",
       "8             0       0       0              1      0               0   \n",
       "9             0       0       0              0      1               0   \n",
       "10            0       0       0              0      0               1   \n",
       "11            0       0       0              0      0               0   \n",
       "\n",
       "    is_berkeley!  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              0  \n",
       "10             0  \n",
       "11             1  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_encoder = OneHotEncoder(categories=[unique_words],sparse_output=False, dtype=int)\n",
    "encoded_oh = oh_encoder.fit_transform(df[['word']].astype(str))\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_oh, columns=[f'is_{word.lower()}' for word in unique_words], index=df.index)\n",
    "\n",
    "df_one_hot = pd.concat([df.reset_index(drop=True), encoded_df], axis=1)\n",
    "df_one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8d773",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "IF you have categorical data WITH inherent order (e.g., small/medium/large, education levels) â†’ Use ordinal\n",
    "- Applied to categorical features (words, categories)\n",
    "- Output is a single integer\n",
    "- Used for features with inherent order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fee95890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ordinal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>learning</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California,</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berkeley!</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  ordinal_id\n",
       "0             I           0\n",
       "1          love           1\n",
       "2    everything           2\n",
       "3     involving           3\n",
       "4       machine           4\n",
       "5      learning           5\n",
       "6           and           6\n",
       "7           the           7\n",
       "8    University           8\n",
       "9            of           9\n",
       "10  California,          10\n",
       "11    Berkeley!          11"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_encoder = OrdinalEncoder(categories=[unique_words],dtype=int)\n",
    "\n",
    "encoded_ord = ord_encoder.fit_transform(df[['word']].astype(str))\n",
    "\n",
    "df_ordinal = df.copy()\n",
    "df_ordinal['ordinal_id'] = encoded_ord.astype(int)\n",
    "\n",
    "df_ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a45b3",
   "metadata": {},
   "source": [
    "These encoders give numerical structure to non-numeric data. However, they rely entirely on human-defined categories. In contrast, LLMs will learn their own encoding rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728889e9",
   "metadata": {},
   "source": [
    "# LLM Encoding & Features: ML similarities & differences\n",
    "\n",
    "## PCA v LLM Latent Features\n",
    "When you apply scaling, PCA, or clustering on numerical data, you're effectively trying to uncover hidden features that aren't visible in your existing features/dataset e.g. PCA explicitly creates **latent features** (principal components) through linear combinations of our existing numerical features. \n",
    "\n",
    "In LLMs, latent features are not computed algorithmically or are assumed to be linear - instead they emerge from the raw data as part of training. \n",
    "\n",
    "In other words, \n",
    "- Traditional ML: **you** *engineer* latent features explicitly --> using tools like PCA or clustering \n",
    "- LLMs: the **model** *discovers* latent features implicitly --> through backwardation\n",
    "  \n",
    "\n",
    "## Encoding text v features\n",
    "\n",
    "In traditional ML, we encode text features using techniques such as ordinal encoding or one-hot encoding to transform the features into a numerical format the model can ingest.\n",
    "\n",
    "Similarly, encoding is used in LLMs to prepare text data prior to model training. The difference is that LLMs begin encoding text data before any features are identified. \n",
    "- *LLMs begin by converting raw text into token IDs (numerical form) before the model learns any features or relationships between them.*\n",
    "\n",
    "That is to say, LLMs identify features implicitly and as part of its training process, not explicitly and as part of the pre-processing process (as we have seen thus far in typical regression and classifier type models)\n",
    "- *LLMs identify features implicitly within their neural layers during training, rather than explicitly during data preprocessing.*\n",
    "\n",
    "Summary:\n",
    "- ML Features: explicit, identified by the Data Scientist before training\n",
    "- LLM Features: implicit, discovered by the Model during training\n",
    "\n",
    "As Data Scientists, we still need to prepare the data for encoding, it's just that we're not encoding features.\n",
    "\n",
    "Letâ€™s visualise how these two approaches â€” explicit feature engineering and implicit feature discovery â€” differ at the pipeline level:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b73134",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "```mermaid\n",
    "\n",
    "flowchart TD\n",
    "    %% --- Traditional ML Branch ---\n",
    "    A[\"Raw Text Data\"] --> B[\"ML Data Preparation\"]\n",
    "    B --> C[\"Feature Engineering<br/>(Explicit, Human-Defined)\"]\n",
    "    C --> D[\"Encoding (Ordinal / One-Hot)\"]\n",
    "    D --> E[\"Model Training<br/>(Learns relationships between engineered features)\"]\n",
    "\n",
    "    %% --- LLM Branch ---\n",
    "    A --> F[\"LLM Data Preparation\"]\n",
    "    F --> G[\"Chunking<br/>(Split corpus into semantically meaningful text units)\"]\n",
    "    G --> H[\"Encoding / Tokenisation<br/>(Convert text â†’ token IDs via Tokeniser model)\"]\n",
    "    H --> I[\"LLM Training<br/>(Learns latent features implicitly within neural layers)\"]\n",
    "\n",
    "    %% --- Tokeniser Details ---\n",
    "    H --> J[\"Pre-trained Tokeniser<br/>(e.g., BPE - Byte Pair Encoding)\"]\n",
    "    J --> K[\"Feature Extraction: identify frequent subword patterns\"]\n",
    "    J --> L[\"Feature Engineering: map subwords â†’ integer token IDs\"]\n",
    "\n",
    "    %% --- Styling / Grouping ---\n",
    "    subgraph Traditional_ML[\"Traditional ML Pipeline\"]\n",
    "        B --> C --> D --> E\n",
    "    end\n",
    "\n",
    "    subgraph LLM[\"LLM Pipeline\"]\n",
    "        F --> G --> H --> I\n",
    "    end\n",
    "\n",
    "    style Traditional_ML fill:#f5f5f5,stroke:#666,stroke-width:1px\n",
    "    style LLM fill:#f5f5f5,stroke:#666,stroke-width:1px\n",
    "    style A fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
    "    style E fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "    style I fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "    style J fill:#e3f2fd,stroke:#1565c0,stroke-width:1px\n",
    "    style K fill:#bbdefb,stroke:#1565c0,stroke-width:1px\n",
    "    style L fill:#bbdefb,stroke:#1565c0,stroke-width:1px\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb7ae9",
   "metadata": {},
   "source": [
    "## LLM Data Preparation: Chunking\n",
    "\n",
    "To help an LLM discover these text features, the first step before training any LLM is to break down their enormous training dataset (corpus) into manageable chunks for encoding and then training.  \n",
    "\n",
    "Chunks are typically semantically meaningful bodies of text - which can range between an entire document or sentence depending on the training objective. The limitation of the chunk size is called the LLMs context window\n",
    "\n",
    "After breaking down the corpus into chunks, we then need to break down the chunks into small, unique strings we can then encode.\n",
    "\n",
    "## Encoding small strings\n",
    "\n",
    "These small, unique strings are frequently appearing character combinations in languages - often representing words or sub-words. Once identified, these can be mapped to a unique ID and subsequently encoded in the text by replacing the string with the ID. \n",
    "\n",
    "In practise, this encoding process is so time-consuming and complicated (how can you guarantee the sub-string and ID matches don't change when you update your training corpus?!?!) that it requires a model of its own. \n",
    "\n",
    "As a result, most LLMS use pre-trained encoders created from other models.\n",
    "\n",
    "These models are referred to as 'Tokenisers'.\n",
    "\n",
    "\n",
    "## Tokenisers: text encoding models for LLMs\n",
    "\n",
    "A \"Tokenizer\" is simply a model that CONSISTENTLY encodes your text data into a numerical format for LLM ingestion. This process is called tokenisation. Under the hood, the most popular algo for tokenisation is BPE.\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a popular encoding algo used in tokenisers that seeks to extract and engineer features. \n",
    "\n",
    "BPE seeks to:\n",
    "- identify the most frequent combinations of characters in your text dataset as features â†’ **feature extraction**\n",
    "- map each feature to a unique integer (token id) suitable for model training â†’ **feature engineering**\n",
    "\n",
    "Want to visualise text tokenisation in OpenAI models? [Tiktokenizer ](https://tiktokenizer.vercel.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e91a5",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% --- INPUT STAGE ---\n",
    "    A[\"â‘  Raw Text Corpus<br/>(e.g. 'machine learning models are amazing')\"] --> B[\"â‘¡ Select Tokenization Algorithm<br/>(e.g. BPE, Unigram, WordPiece)\"]\n",
    "    B --> C[\"â‘¢ Initialize Vocabulary<br/>(Each character = token)<br/>Vocabulary size = 256\"]\n",
    "\n",
    "    %% --- FEATURE EXTRACTION STAGE ---\n",
    "    C --> D[\"â‘£ Identify Frequent Pairs<br/>(Find most common adjacent characters/subwords)\"]\n",
    "    D --> E[\"â‘¤ Merge Frequent Pairs<br/>(Combine 'm' + 'a' â†’ 'ma', add new token)\"]\n",
    "    E --> F[\"â‘¥ Update Vocabulary<br/>(Add merged token with new Token ID)\"]\n",
    "\n",
    "    %% --- FEATURE ENGINEERING STAGE ---\n",
    "    F --> G[\"â‘¦ Replace Original Tokens<br/>(Replace 'm' 'a' with 'ma' Token ID)\"]\n",
    "    G --> H[\"â‘§ Repeat Merging Iteratively<br/>Until no frequent pairs remain\"]\n",
    "\n",
    "    %% --- OUTPUT STAGE ---\n",
    "    H --> I[\"â‘¨ Final Vocabulary<br/>(Subwords / Character Pairs + Unique Token IDs)\"]\n",
    "\n",
    "    %% --- STYLING ---\n",
    "    style A fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
    "    style B fill:#ffe0b2,stroke:#f57c00,stroke-width:1px\n",
    "    style C fill:#fff9c4,stroke:#f9a825,stroke-width:1px\n",
    "    style D fill:#e3f2fd,stroke:#1565c0,stroke-width:1px\n",
    "    style E fill:#bbdefb,stroke:#1565c0,stroke-width:1px\n",
    "    style F fill:#90caf9,stroke:#1565c0,stroke-width:1px\n",
    "    style G fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "    style H fill:#c8e6c9,stroke:#2e7d32,stroke-width:1px\n",
    "    style I fill:#aed581,stroke:#2e7d32,stroke-width:1px\n",
    "\n",
    "    %% --- GROUP LABELS ---\n",
    "    subgraph TI[\"ðŸ§© TOKENIZER INPUTS\"]\n",
    "        A --> B --> C\n",
    "    end\n",
    "\n",
    "    subgraph FE[\"ðŸ§  Feature Extraction Phase\"]\n",
    "        D --> E --> F\n",
    "    end\n",
    "\n",
    "    subgraph EN[\"âš™ï¸ Feature Engineering Phase\"]\n",
    "        G --> H\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c87f3",
   "metadata": {},
   "source": [
    "#### 1. Data Collection & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c930c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = pathlib.Path(\"../datasets\")\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = \"https://em-executive.berkeley.edu/professional-certificate-machine-learning-artificial-intelligence\"\n",
    "\n",
    "corpus_source = requests.get(url)\n",
    "corpus_source.raise_for_status()\n",
    "soup = BeautifulSoup(corpus_source.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "full_text = \"\\n\".join(paragraphs)\n",
    "\n",
    "raw_file = dataset_dir / \"corpus_full.txt\"\n",
    "with raw_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5380a57",
   "metadata": {},
   "source": [
    "### 2: Tokenizer Algorithm\n",
    "Select the tokenizer algorightm and any pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af9976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcc1ed",
   "metadata": {},
   "source": [
    "### 3-8: Tokenizer Training\n",
    "\n",
    "Now that our text corpus is saved, letâ€™s train our own BPE-based tokenizer. This is the same type of process used by models like GPT, except on a massive scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "110ffe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],vocab_size=2000)\n",
    "\n",
    "tokenizer.train(files=[str(raw_file)], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674403f3",
   "metadata": {},
   "source": [
    "### 9. Final Vocabulary\n",
    "That's it. You have now created a \"vocabulary\" containing a token (string): token id for every frequent combination of characters it could find!\n",
    "\n",
    "Note: as languages increase with words, use cases get more defined (for LLMs), and models become more powerful, I expect vocab sizes to increase to incorporate more whole words e.g. LinkedIn, Professor etc.\n",
    "\n",
    "Below is a sample of the tokens our tokenizer learned â€” these represent frequent subword units discovered from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "60178d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1661\n",
      "900: 04\n",
      "1073: nal\n",
      "220: Education\n",
      "207: ecutive\n",
      "578: Colle\n",
      "1293: learners\n",
      "68: n\n",
      "703: online\n",
      "209: port\n",
      "797: Develop\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "for token, idx in list(vocab.items())[:10]:\n",
    "    print(f\"{idx}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a896cb1",
   "metadata": {},
   "source": [
    "# Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde39683",
   "metadata": {},
   "source": [
    "## Text Encoding comparison: ML v LLM\n",
    "\n",
    "Letâ€™s now merge our outputs â€” one-hot, ordinal, and BPE â€” into a single table to visually compare how traditional and LLM encodings differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ad0835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>token_id</th>\n",
       "      <th>ordinal_id</th>\n",
       "      <th>is_i</th>\n",
       "      <th>is_love</th>\n",
       "      <th>is_everything</th>\n",
       "      <th>is_involving</th>\n",
       "      <th>is_machine</th>\n",
       "      <th>is_learning</th>\n",
       "      <th>is_and</th>\n",
       "      <th>is_the</th>\n",
       "      <th>is_university</th>\n",
       "      <th>is_of</th>\n",
       "      <th>is_california,</th>\n",
       "      <th>is_berkeley!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>[39]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>[137, 114]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>[1024, 79, 86, 98]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>[83, 76, 722]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine</td>\n",
       "      <td>[750]</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>learning</td>\n",
       "      <td>[448]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and</td>\n",
       "      <td>[96]</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>[99]</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University</td>\n",
       "      <td>[842]</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>[103]</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California,</td>\n",
       "      <td>[892, 13]</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berkeley!</td>\n",
       "      <td>[133, 5]</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word            token_id  ordinal_id  is_i  is_love  is_everything  \\\n",
       "0             I                [39]           0     1        0              0   \n",
       "1          love          [137, 114]           1     0        1              0   \n",
       "2    everything  [1024, 79, 86, 98]           2     0        0              1   \n",
       "3     involving       [83, 76, 722]           3     0        0              0   \n",
       "4       machine               [750]           4     0        0              0   \n",
       "5      learning               [448]           5     0        0              0   \n",
       "6           and                [96]           6     0        0              0   \n",
       "7           the                [99]           7     0        0              0   \n",
       "8    University               [842]           8     0        0              0   \n",
       "9            of               [103]           9     0        0              0   \n",
       "10  California,           [892, 13]          10     0        0              0   \n",
       "11    Berkeley!            [133, 5]          11     0        0              0   \n",
       "\n",
       "    is_involving  is_machine  is_learning  is_and  is_the  is_university  \\\n",
       "0              0           0            0       0       0              0   \n",
       "1              0           0            0       0       0              0   \n",
       "2              0           0            0       0       0              0   \n",
       "3              1           0            0       0       0              0   \n",
       "4              0           1            0       0       0              0   \n",
       "5              0           0            1       0       0              0   \n",
       "6              0           0            0       1       0              0   \n",
       "7              0           0            0       0       1              0   \n",
       "8              0           0            0       0       0              1   \n",
       "9              0           0            0       0       0              0   \n",
       "10             0           0            0       0       0              0   \n",
       "11             0           0            0       0       0              0   \n",
       "\n",
       "    is_of  is_california,  is_berkeley!  \n",
       "0       0               0             0  \n",
       "1       0               0             0  \n",
       "2       0               0             0  \n",
       "3       0               0             0  \n",
       "4       0               0             0  \n",
       "5       0               0             0  \n",
       "6       0               0             0  \n",
       "7       0               0             0  \n",
       "8       0               0             0  \n",
       "9       1               0             0  \n",
       "10      0               1             0  \n",
       "11      0               0             1  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = []\n",
    "for word in unique_words:\n",
    "    encoded = tokenizer.encode(word)\n",
    "    token_ids.append(encoded.ids)  # .ids gives the integer token IDs\n",
    "\n",
    "# --- Build DataFrame ---\n",
    "df_bpe = pd.DataFrame({\n",
    "    \"word\": unique_words,\n",
    "    \"token_id\": token_ids\n",
    "})\n",
    "\n",
    "\n",
    "df_encode = df_bpe.copy()\n",
    "df_encode = df_encode.merge(df_ordinal, on='word')\n",
    "df_encode = df_encode.merge(df_one_hot, on='word')\n",
    "df_encode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6c2aa",
   "metadata": {},
   "source": [
    "### Lightbulb moment\n",
    "What is cool about this? In our original sentence, the word \"involving\" (idx: 3) is tokenised BUT doesn't exist in the corpus!\n",
    "\n",
    "Instead, the string:\n",
    "- \"inv\" appeared 8 times in the corpus from the word \"invite\" --> encoded with token id: 83\n",
    "- \"olv\" appeared 6 times in the corpus, from words such as \"solve\" and \"evolve\" --> encoded with token id: 76\n",
    "- \"ing\" appeared 152 times in the corpus, from lots of words! --> encoded with token id 722"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6993b",
   "metadata": {},
   "source": [
    "## Putting it all together: how an LLM encodes during inference\n",
    "\n",
    "We discussed a lot about feature engineering and encoding today - but this is essentially what happens when every time you run a prompt/query whenever you use an LLM such as ChatGPT!\n",
    "\n",
    "User Input: \"I love everything involving machine learning and the University of California, Berkeley!\"\n",
    "\n",
    "LLM's tokenizer: breaks this text into subword tokens using greedy longest-match-first matching against its  vocabulary, matches it against a token, finds its token ID...(see diagram below for a fuller picture)\n",
    "\n",
    "\n",
    "**The takeaway:** whether youâ€™re engineering one-hot vectors or training massive LLMs, both rely on feature encoding â€” the key difference lies in who engineers the features: you, or the model.\n",
    "\n",
    "\n",
    "Have a great day!\n",
    "\n",
    "<center>\n",
    "\n",
    "\n",
    "``` mermaid\n",
    "flowchart TD\n",
    "    %% --- User Input ---\n",
    "    A[\"<b>â‘  User Input</b><br/>'I love everything involving machine learning...'\"]\n",
    "\n",
    "    %% --- Tokenizer Stage ---\n",
    "    A --> B[\"<b>â‘¡ Tokenizer: Identify Tokens</b><br/>(Greedy Longest-Match-First Search)\"]\n",
    "    B --> C[\"<b>â‘¢ Tokenizer: Vocabulary Lookup</b><br/>(Each Token â†’ Token ID)\"]\n",
    "\n",
    "    %% --- LLM Input Processing ---\n",
    "    C --> D[\"â‘£ LLM Input Layer: Embedding Lookup<br/>(Token IDs â†’ Embedding Vectors)\"]\n",
    "    D --> E[\"â‘¤ LLM Input Layer: Positional Encoding<br/>(Add Sequence Order Info to Vectors)\"]\n",
    "\n",
    "    %% --- LLM Learning ---\n",
    "    E --> F[\"â‘¥ RUN MODEL!<br/>(to be continued...)\"]\n",
    "\n",
    "    %% --- Today's Lesson ---\n",
    "    subgraph TL[\"ðŸ§  Discussed today\"]\n",
    "        A --> B --> C\n",
    "    end\n",
    "\n",
    "    %% --- Styling ---\n",
    "    style TL fill:#fff3e0,stroke:#f57c00,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style A fill:#fffde7,stroke:#f9a825,stroke-width:1px\n",
    "    style B fill:#e3f2fd,stroke:#1565c0,stroke-width:1px\n",
    "    style C fill:#bbdefb,stroke:#1565c0,stroke-width:1px\n",
    "    style D fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "    style E fill:#c8e6c9,stroke:#2e7d32,stroke-width:1px\n",
    "    style F fill:#f1f8e9,stroke:#2e7d32,stroke-width:1px\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
