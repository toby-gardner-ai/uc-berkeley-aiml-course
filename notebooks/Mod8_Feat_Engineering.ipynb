{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0dd43b9",
   "metadata": {},
   "source": [
    "# Feature/Data Engineering: encoding\n",
    "\n",
    "In our last session, we expanded on the curriculum by looking deeper on how to apply unsupervised learning techniques such as DBScan Clustering and PCA not only for EDA and dimensionality reduction but also on how we can create new features aka feature engineering. If you're interested in reviewing feature engineering for numerical data (with examples), check out:\n",
    "- notebooks/Mod4_Data_Analytics.ipynb\n",
    "- notebooks/Mod6_PCA_Clustering.ipynb\n",
    "\n",
    "In this session, we're going to explore Feature Engineering for categorical features and text-based data: preparing you not only in traditional ML but also for Generative AI such as LLMs.\n",
    "\n",
    "Before we dive into encoding (tokenization) for LLMs, let’s revisit how we handle feature encoding for categorical data — a crucial step that parallels what LLMs later do implicitly with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac8362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from IPython.display import Image, HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9432f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent=pathlib.Path.cwd().parent\n",
    "sys.path.insert(0,str(parent))\n",
    "\n",
    "from helpers.visuals import show_mermaid_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d8821",
   "metadata": {},
   "source": [
    "Let's create a simple sentence, split it into words, and capture the unique words in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dc0d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love everything involving AI and machine learning and love the University of California, Berkeley!\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "# dict.fromkeys() --> keeps order and the first time each word appears\n",
    "unique_words = list(dict.fromkeys(words))\n",
    "\n",
    "df = pd.DataFrame(words, columns=['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294297a",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "IF you have categorical data WITH inherent order (e.g., small/medium/large, education levels) → Use ordinal\n",
    "- Applied to categorical features (words, categories)\n",
    "- Output is a single integer\n",
    "- Used for features with inherent order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cefbc034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ordinal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>learning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>University</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California,</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Berkeley!</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  ordinal_id\n",
       "0             I           0\n",
       "1          love           1\n",
       "2    everything           2\n",
       "3     involving           3\n",
       "4            AI           4\n",
       "5           and           5\n",
       "6       machine           6\n",
       "7      learning           7\n",
       "8           and           5\n",
       "9          love           1\n",
       "10          the           8\n",
       "11   University           9\n",
       "12           of          10\n",
       "13  California,          11\n",
       "14    Berkeley!          12"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_encoder = OrdinalEncoder(categories=[unique_words],dtype=int)\n",
    "\n",
    "encoded_ord = ord_encoder.fit_transform(df[['word']].astype(str))\n",
    "\n",
    "df_ordinal = df.copy()\n",
    "df_ordinal['ordinal_id'] = encoded_ord.astype(int)\n",
    "\n",
    "df_ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723dbb61",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "Using one-hot encoding and a dataframe, we can create and visualise the occurrence of each unique word (categorical data), now as a feature column. \n",
    "Tip: IF you have categorical data with NO inherent order (e.g., colors, countries) → Use one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4e566be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>_i</th>\n",
       "      <th>_love</th>\n",
       "      <th>_everything</th>\n",
       "      <th>_involving</th>\n",
       "      <th>_ai</th>\n",
       "      <th>_and</th>\n",
       "      <th>_machine</th>\n",
       "      <th>_learning</th>\n",
       "      <th>_the</th>\n",
       "      <th>_university</th>\n",
       "      <th>_of</th>\n",
       "      <th>_california,</th>\n",
       "      <th>_berkeley!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>learning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>University</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California,</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Berkeley!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  _i  _love  _everything  _involving  _ai  _and  _machine  \\\n",
       "0             I   1      0            0           0    0     0         0   \n",
       "1          love   0      1            0           0    0     0         0   \n",
       "2    everything   0      0            1           0    0     0         0   \n",
       "3     involving   0      0            0           1    0     0         0   \n",
       "4            AI   0      0            0           0    1     0         0   \n",
       "5           and   0      0            0           0    0     1         0   \n",
       "6       machine   0      0            0           0    0     0         1   \n",
       "7      learning   0      0            0           0    0     0         0   \n",
       "8           and   0      0            0           0    0     1         0   \n",
       "9          love   0      1            0           0    0     0         0   \n",
       "10          the   0      0            0           0    0     0         0   \n",
       "11   University   0      0            0           0    0     0         0   \n",
       "12           of   0      0            0           0    0     0         0   \n",
       "13  California,   0      0            0           0    0     0         0   \n",
       "14    Berkeley!   0      0            0           0    0     0         0   \n",
       "\n",
       "    _learning  _the  _university  _of  _california,  _berkeley!  \n",
       "0           0     0            0    0             0           0  \n",
       "1           0     0            0    0             0           0  \n",
       "2           0     0            0    0             0           0  \n",
       "3           0     0            0    0             0           0  \n",
       "4           0     0            0    0             0           0  \n",
       "5           0     0            0    0             0           0  \n",
       "6           0     0            0    0             0           0  \n",
       "7           1     0            0    0             0           0  \n",
       "8           0     0            0    0             0           0  \n",
       "9           0     0            0    0             0           0  \n",
       "10          0     1            0    0             0           0  \n",
       "11          0     0            1    0             0           0  \n",
       "12          0     0            0    1             0           0  \n",
       "13          0     0            0    0             1           0  \n",
       "14          0     0            0    0             0           1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_encoder = OneHotEncoder(categories=[unique_words], sparse_output=False, dtype=int, handle_unknown=\"ignore\")\n",
    "\n",
    "encoded_oh = oh_encoder.fit_transform(df[['word']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_oh, columns=[f'_{word.lower()}' for word in unique_words], index=df.index)\n",
    "\n",
    "df_one_hot = pd.concat([df.reset_index(drop=True), encoded_df], axis=1)\n",
    "df_one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08772a2",
   "metadata": {},
   "source": [
    "## Advanced: From Text --> Categorical Data --> One-Hot Encoding\n",
    "\n",
    "So far, we’ve treated words themselves as categories and encoded them directly - but in the real-world we often want to transform raw text into higher-level categorical features before encoding. \n",
    "\n",
    "One way to create categorical features from raw text (and then encode) is to categorise words into nouns, verbs, adjectives etc. aka part of speech (POS)\n",
    "\n",
    "In Natural Language Processing (NLP), which we will cover later, you can do this using a library like spacy. \n",
    "\n",
    "*Note: when updating for all installs (pip install -r requirements.txt) to install spacy you will still need to run the command: python -m spacy download en_core_web_sm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0c89fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>_pron</th>\n",
       "      <th>_verb</th>\n",
       "      <th>_propn</th>\n",
       "      <th>_cconj</th>\n",
       "      <th>_noun</th>\n",
       "      <th>_det</th>\n",
       "      <th>_adp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>learning</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>University</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Berkeley</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word    pos  _pron  _verb  _propn  _cconj  _noun  _det  _adp\n",
       "0            I   PRON      1      0       0       0      0     0     0\n",
       "1         love   VERB      0      1       0       0      0     0     0\n",
       "2   everything   PRON      1      0       0       0      0     0     0\n",
       "3    involving   VERB      0      1       0       0      0     0     0\n",
       "4           AI  PROPN      0      0       1       0      0     0     0\n",
       "5          and  CCONJ      0      0       0       1      0     0     0\n",
       "6      machine   NOUN      0      0       0       0      1     0     0\n",
       "7     learning   NOUN      0      0       0       0      1     0     0\n",
       "8          and  CCONJ      0      0       0       1      0     0     0\n",
       "9         love   VERB      0      1       0       0      0     0     0\n",
       "10         the    DET      0      0       0       0      0     1     0\n",
       "11  University  PROPN      0      0       1       0      0     0     0\n",
       "12          of    ADP      0      0       0       0      0     0     1\n",
       "13  California  PROPN      0      0       1       0      0     0     0\n",
       "14    Berkeley  PROPN      0      0       1       0      0     0     0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorise_text = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = categorise_text(sentence)\n",
    "\n",
    "words = []\n",
    "pos_categ = []\n",
    "\n",
    "for token in text:\n",
    "    if token.is_space or token.is_punct:\n",
    "        continue\n",
    "    words.append(token.text)\n",
    "    pos_categ.append(token.pos_)\n",
    "\n",
    "df_pos = pd.DataFrame({'word': words,'pos': pos_categ})\n",
    "\n",
    "unique_pos = list(dict.fromkeys(pos_categ))\n",
    "\n",
    "oh_encoder = OneHotEncoder(categories=[unique_pos], sparse_output=False, dtype=int, handle_unknown=\"ignore\")\n",
    "\n",
    "encoded_oh = oh_encoder.fit_transform(df_pos[['pos']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_oh, columns=[f'_{pos.lower()}' for pos in unique_pos], index=df_pos.index)\n",
    "\n",
    "df_pos_ohe = pd.concat([df_pos.reset_index(drop=True), encoded_df], axis=1)\n",
    "df_pos_ohe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a45b3",
   "metadata": {},
   "source": [
    "These encoders give numerical structure to non-numeric data. However, they rely entirely on human-defined categories. In contrast, LLMs will learn their own encoding rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728889e9",
   "metadata": {},
   "source": [
    "# LLM Encoding & Features: ML similarities & differences\n",
    "\n",
    "## PCA v LLM Latent Features\n",
    "When you apply scaling, PCA, or clustering on numerical data, you're effectively trying to uncover hidden features that aren't visible in your existing features/dataset e.g. PCA explicitly creates **latent features** (principal components) through linear combinations of our existing numerical features. \n",
    "\n",
    "In LLMs, latent features are not computed algorithmically or are assumed to be linear - instead they emerge from the raw data as part of training. \n",
    "\n",
    "In other words, \n",
    "- Traditional ML: **you** *engineer* latent features explicitly --> using tools like PCA or clustering \n",
    "- LLMs: the **model** *discovers* latent features implicitly --> through backwardation\n",
    "  \n",
    "\n",
    "## Encoding text v features\n",
    "\n",
    "In traditional ML, we encode text features using techniques such as ordinal encoding or one-hot encoding to transform the features into a numerical format the model can ingest.\n",
    "\n",
    "Similarly, encoding is used in LLMs to prepare text data prior to model training. The difference is that LLMs begin encoding text data before any features are identified. \n",
    "- *LLMs begin by converting raw text into token IDs (numerical form) before the model learns any features or relationships between them.*\n",
    "\n",
    "That is to say, LLMs identify features implicitly and as part of its training process, not explicitly and as part of the pre-processing process (as we have seen thus far in typical regression and classifier type models)\n",
    "- *LLMs identify features implicitly within their neural layers during training, rather than explicitly during data preprocessing.*\n",
    "\n",
    "Summary:\n",
    "- ML Features: explicit, identified by the Data Scientist before training\n",
    "- LLM Features: implicit, discovered by the Model during training\n",
    "\n",
    "As Data Scientists, we still need to prepare the data for encoding, it's just that we're not encoding features.\n",
    "\n",
    "Let’s visualise how these two approaches — explicit feature engineering and implicit feature discovery — differ at the pipeline level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42584103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='text-align:center;'><img src=\"/Users/tobygardner/Projects/uc-berkeley-aiml-course/images/ml_llm_data_prep_pipeline.png\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_mermaid_diagram(\"ml_llm_data_prep_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb7ae9",
   "metadata": {},
   "source": [
    "## LLM Data Preparation: Chunking\n",
    "\n",
    "To help an LLM discover these text features, the first step before training any LLM is to break down their enormous training dataset (corpus) into manageable chunks for encoding and then training.  \n",
    "\n",
    "Chunks are typically semantically meaningful bodies of text - which can range between an entire document or sentence depending on the training objective. The limitation of the chunk size is called the LLMs context window\n",
    "\n",
    "After breaking down the corpus into chunks, we then need to break down the chunks into small, unique strings we can then encode.\n",
    "\n",
    "## Encoding small strings\n",
    "\n",
    "These small, unique strings are frequently appearing character combinations in languages - often representing words or sub-words. Once identified, these can be mapped to a unique ID and subsequently encoded in the text by replacing the string with the ID. \n",
    "\n",
    "In practise, this encoding process is so time-consuming and complicated (how can you guarantee the sub-string and ID matches don't change when you update your training corpus?!?!) that it requires a model of its own. \n",
    "\n",
    "As a result, most LLMS use pre-trained encoders created from other models.\n",
    "\n",
    "These models are referred to as 'Tokenisers'.\n",
    "\n",
    "\n",
    "## Tokenisers: text encoding models for LLMs\n",
    "\n",
    "A \"Tokenizer\" is simply a model that CONSISTENTLY encodes your text data into a numerical format for LLM ingestion. This process is called tokenisation. Under the hood, the most popular algo for tokenisation is BPE.\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a popular encoding algo used in tokenisers that seeks to extract and engineer features. \n",
    "\n",
    "BPE seeks to:\n",
    "- identify the most frequent combinations of characters in your text dataset as features → **feature extraction**\n",
    "- map each feature to a unique integer (token id) suitable for model training → **feature engineering**\n",
    "\n",
    "Want to visualise text tokenisation in OpenAI models? [Tiktokenizer ](https://tiktokenizer.vercel.app/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb79e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='text-align:center;'><img src=\"/Users/tobygardner/Projects/uc-berkeley-aiml-course/images/tokenizer_process.png\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_mermaid_diagram(\"tokenizer_process\", center=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c87f3",
   "metadata": {},
   "source": [
    "#### 1. Data Collection & Cleaning\n",
    "\n",
    "Let's create our corpus by scraping text from this courses description at UC Berkeley's Executive Education Program: https://em-executive.berkeley.edu/professional-certificate-machine-learning-artificial-intelligence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41c930c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = pathlib.Path(\"../datasets\")\n",
    "\n",
    "\n",
    "url = \"https://em-executive.berkeley.edu/professional-certificate-machine-learning-artificial-intelligence\"\n",
    "\n",
    "corpus_source = requests.get(url)\n",
    "corpus_source.raise_for_status()\n",
    "soup = BeautifulSoup(corpus_source.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1864144",
   "metadata": {},
   "source": [
    "Let's do some basic cleaning and joining of our corpus data\n",
    "Note: data cleaning isn't as important here as it is during normal model data preparation. This is because we're less interested in the semantic meaning and data quality as we are the combinations of characters in our text to create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b90b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "full_text = \"\\n\".join(paragraphs)\n",
    "\n",
    "raw_file = dataset_dir / \"corpus_full.txt\"\n",
    "with raw_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5380a57",
   "metadata": {},
   "source": [
    "### 2: Tokenizer Algorithm\n",
    "Select the tokenizer algorightm and any pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40af9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcc1ed",
   "metadata": {},
   "source": [
    "### 3-8: Tokenizer Training\n",
    "\n",
    "Now that our text corpus is saved, let’s train our own BPE-based tokenizer. This is the same type of process used by models like GPT, except on a massive scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "110ffe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],vocab_size=2000)\n",
    "\n",
    "tokenizer.train(files=[str(raw_file)], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674403f3",
   "metadata": {},
   "source": [
    "### 9. Final Vocabulary\n",
    "That's it. You have now created a \"vocabulary\" containing a token (string): token id for every frequent combination of characters it could find!\n",
    "\n",
    "Note: as languages increase with words, use cases get more defined (for LLMs), and models become more powerful, I expect vocab sizes to increase to incorporate more whole words e.g. LinkedIn, Professor etc.\n",
    "\n",
    "Below is a sample of the tokens our tokenizer learned — these represent frequent subword units discovered from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60178d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1661\n",
      "1080: oking\n",
      "1452: email\n",
      "252: gh\n",
      "512: sci\n",
      "898: ),\n",
      "337: job\n",
      "1096: sch\n",
      "1500: 9495\n",
      "1566: librar\n",
      "1133: ze\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "for token, idx in list(vocab.items())[:10]:\n",
    "    print(f\"{idx}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a896cb1",
   "metadata": {},
   "source": [
    "# Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde39683",
   "metadata": {},
   "source": [
    "## Text Encoding comparison: ML v LLM\n",
    "\n",
    "Let’s now merge our outputs — one-hot, ordinal, and BPE — into a single table to visually compare how traditional and LLM encodings differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ad0835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>token_id</th>\n",
       "      <th>_pron</th>\n",
       "      <th>_verb</th>\n",
       "      <th>_propn</th>\n",
       "      <th>_cconj</th>\n",
       "      <th>_noun</th>\n",
       "      <th>_det</th>\n",
       "      <th>_adp</th>\n",
       "      <th>ordinal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>[39]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>[137, 114]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everything</td>\n",
       "      <td>[1024, 79, 86, 98]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>involving</td>\n",
       "      <td>[83, 76, 722]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>[134]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>[96]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>[750]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>learning</td>\n",
       "      <td>[448]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>[96]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>[137, 114]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>[99]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>University</td>\n",
       "      <td>[842]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>[103]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word            token_id  _pron  _verb  _propn  _cconj  _noun  _det  \\\n",
       "0            I                [39]      1      0       0       0      0     0   \n",
       "1         love          [137, 114]      0      1       0       0      0     0   \n",
       "2   everything  [1024, 79, 86, 98]      1      0       0       0      0     0   \n",
       "3    involving       [83, 76, 722]      0      1       0       0      0     0   \n",
       "4           AI               [134]      0      0       1       0      0     0   \n",
       "5          and                [96]      0      0       0       1      0     0   \n",
       "6      machine               [750]      0      0       0       0      1     0   \n",
       "7     learning               [448]      0      0       0       0      1     0   \n",
       "8          and                [96]      0      0       0       1      0     0   \n",
       "9         love          [137, 114]      0      1       0       0      0     0   \n",
       "10         the                [99]      0      0       0       0      0     1   \n",
       "11  University               [842]      0      0       1       0      0     0   \n",
       "12          of               [103]      0      0       0       0      0     0   \n",
       "\n",
       "    _adp  ordinal_id  \n",
       "0      0           0  \n",
       "1      0           1  \n",
       "2      0           2  \n",
       "3      0           3  \n",
       "4      0           4  \n",
       "5      0           5  \n",
       "6      0           6  \n",
       "7      0           7  \n",
       "8      0           5  \n",
       "9      0           1  \n",
       "10     0           8  \n",
       "11     0           9  \n",
       "12     1          10  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = []\n",
    "for word in words:\n",
    "    encoded = tokenizer.encode(word)\n",
    "    token_ids.append(encoded.ids)\n",
    "\n",
    "df_bpe = pd.DataFrame({\"word\": words,\"token_id\": token_ids})\n",
    "\n",
    "df_ordinal[\"idx\"] = df_ordinal.index\n",
    "df_pos_ohe[\"idx\"] = df_pos_ohe.index\n",
    "df_bpe[\"idx\"]      = df_bpe.index\n",
    "\n",
    "\n",
    "df_encode = (df_bpe.merge(df_pos_ohe, on=[\"idx\", \"word\"]).drop(columns=\"pos\"))\n",
    "\n",
    "\n",
    "df_encode = (df_encode.merge(df_ordinal, on=[\"idx\", \"word\"]).drop(columns=\"idx\"))\n",
    "\n",
    "df_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6c2aa",
   "metadata": {},
   "source": [
    "### Lightbulb moment\n",
    "What is cool about this? In our original sentence, the word \"involving\" (idx: 3) is tokenised BUT doesn't exist in the corpus!\n",
    "\n",
    "Instead, the string:\n",
    "- \"inv\" appeared 8 times in the corpus from the word \"invite\" --> encoded with token id: 83\n",
    "- \"olv\" appeared 6 times in the corpus, from words such as \"solve\" and \"evolve\" --> encoded with token id: 76\n",
    "- \"ing\" appeared 152 times in the corpus, from lots of words! --> encoded with token id 722"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6993b",
   "metadata": {},
   "source": [
    "# (Out of Scope) LLM Encoding during inference\n",
    "\n",
    "We discussed a lot about feature engineering and encoding today - but this is essentially what happens when every time you run a prompt/query whenever you use an LLM such as ChatGPT!\n",
    "\n",
    "User Input: \"I love everything involving machine learning and the University of California, Berkeley!\"\n",
    "\n",
    "LLM's tokenizer: breaks this text into subword tokens using greedy longest-match-first matching against its  vocabulary, matches it against a token, finds its token ID...(see diagram below for a fuller picture)\n",
    "\n",
    "\n",
    "**The takeaway:** whether you’re engineering one-hot vectors or training massive LLMs, both rely on feature encoding — the key difference lies in who engineers the features: you, or the model.\n",
    "\n",
    "\n",
    "Have a great day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b279e5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='text-align:center;'><img src=\"/Users/tobygardner/Projects/uc-berkeley-aiml-course/images/llm_encoding.png\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_mermaid_diagram(\"llm_encoding\", center=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
